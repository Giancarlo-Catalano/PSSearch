{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-26T14:33:44.286842700Z",
     "start_time": "2025-05-26T14:33:40.669142Z"
    }
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "from typing import Iterable, Callable\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "import utils\n",
    "from Core.PRef import PRef\n",
    "from Core.PS import PS\n",
    "from Gian_experimental.NSGAIICustom.NSGAIICustom import NSGAIICustom, NCSolution, NCSamplerSimple, NCMutationSimple, \\\n",
    "    NCCrossoverSimple, EvaluatedNCSolution, NCSamplerFromPRef, NCCrossoverTransition, NCMutation, \\\n",
    "    NCMutationCounterproductive\n",
    "from PolishSystem.OperatorsBasedOnSimilarities.similarities_utils import get_transition_matrix\n",
    "from PolishSystem.OperatorsBasedOnSimilarities.similarities_utils import gian_get_similarities\n",
    "from PolishSystem.read_data import get_pRef_from_vectors\n",
    "\n",
    "\n",
    "def make_similarity_atomicity(similarities):\n",
    "    def atomicity(ps):\n",
    "        if len(ps) < 2:\n",
    "            return -1000\n",
    "        else:\n",
    "            linkages = [similarities[a, b] for a, b in itertools.combinations(ps, r=2)]\n",
    "            return np.average(linkages)\n",
    "\n",
    "    return atomicity\n",
    "\n",
    "\n",
    "def get_matches_non_matches_in_pRef(ps, given_pRef: PRef):\n",
    "    ps_true_values = np.full(shape=250, fill_value=-1, dtype=int)\n",
    "    ps_true_values[list(ps)] = 1\n",
    "    return given_pRef.fitnesses_of_observations_and_complement(PS(ps_true_values))\n",
    "\n",
    "\n",
    "def make_consistency_metric_with_sample_size(pRef: PRef,\n",
    "                                             threshold: float = 0.5,\n",
    "                                             must_match_at_least: int = 3):\n",
    "    def consistency_and_sample(ps):\n",
    "        # matches, non_matches = sPRef.get_matching_fitnesses_and_not_matching(ps, threshold=threshold)\n",
    "        matches, non_matches = get_matches_non_matches_in_pRef(ps, pRef)\n",
    "        if min(len(matches), len(non_matches)) < must_match_at_least:\n",
    "            return 1, len(matches)\n",
    "        else:\n",
    "            test = mannwhitneyu(matches, non_matches, alternative=\"greater\", method=\"asymptotic\")\n",
    "            return test.pvalue, len(matches)\n",
    "            # return permutation_mannwhitney_u(matches, non_matches, n_permutations=50), len(matches)\n",
    "\n",
    "    return consistency_and_sample\n",
    "\n",
    "\n",
    "def make_min_metric_with_sample_size(pRef: PRef):\n",
    "    def min_and_sample(ps):\n",
    "        matches, non_matches = get_matches_non_matches_in_pRef(ps, pRef)\n",
    "        if min(len(matches), len(non_matches)) < 1:\n",
    "            return (-1000, len(matches))\n",
    "        else:\n",
    "            lowest_fitness = np.min(matches)\n",
    "        return lowest_fitness, len(matches)\n",
    "\n",
    "    return min_and_sample\n",
    "\n",
    "\n",
    "class HashedSolution:\n",
    "    solution: NCSolution\n",
    "\n",
    "    def __init__(self,\n",
    "                 sol):\n",
    "        self.solution = sol\n",
    "\n",
    "    def __hash__(self):\n",
    "        return sum(self.solution) % 7787\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.solution == other.solution\n",
    "\n",
    "\n",
    "def make_metrics_cached(metrics):\n",
    "    cached_values = dict()\n",
    "\n",
    "    def get_values(ps):\n",
    "        wrapped = HashedSolution(ps)\n",
    "        if wrapped in cached_values:\n",
    "            return cached_values[wrapped]\n",
    "        else:\n",
    "\n",
    "            value = metrics(ps)\n",
    "            cached_values[wrapped] = value\n",
    "            return value\n",
    "\n",
    "    return get_values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "from Gian_experimental.NSGAIICustom.NSGAIICustom import NSGAIICustom, NCSolution, EvaluatedNCSolution\n",
    "\n",
    "\n",
    "# we want to show how the operators affect the sample_size of the children as the program progresses\n",
    "\n",
    "def run_NSGAII_in_steps(algorithm: NSGAIICustom) -> Iterable[Iterable[EvaluatedNCSolution]]:\n",
    "    # this will yield every generation\n",
    "\n",
    "    used_evaluations = [0]\n",
    "\n",
    "    def with_fitnesses(solution: NCSolution) -> EvaluatedNCSolution:\n",
    "        fitnesses = algorithm.mo_fitness_function(solution)\n",
    "        used_evaluations[0] += 1\n",
    "        return EvaluatedNCSolution(solution, fitnesses)\n",
    "    def sampler_yielder():\n",
    "        while True:\n",
    "            yield with_fitnesses(algorithm.sampling.sample())\n",
    "\n",
    "    algorithm.log(\"Beginning of NC process\")\n",
    "    population = algorithm.make_population(yielder=sampler_yielder(), required_quantity=algorithm.pop_size)\n",
    "    yield population\n",
    "\n",
    "    while (used_evaluations[0] < algorithm.eval_budget):\n",
    "        population = algorithm.make_next_generation(population, with_fitnesses)\n",
    "        yield population\n",
    "        algorithm.log(f\"Used evals: {used_evaluations[0]}\")\n",
    "    pareto_fronts = algorithm.get_pareto_fronts(population)\n",
    "\n",
    "    if algorithm.unique:\n",
    "        return list(set(pareto_fronts[0]))\n",
    "    return pareto_fronts[0]\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-26T14:33:44.305101Z",
     "start_time": "2025-05-26T14:33:44.294839300Z"
    }
   },
   "id": "3bb01745440381d8"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "dir_250 = r\"C:\\Users\\gac8\\PycharmProjects\\PSSearch\\data\\retail_forecasting\\250\"\n",
    "\n",
    "\n",
    "def in_250(path):\n",
    "    return os.path.join(dir_250, path)\n",
    "\n",
    "\n",
    "def count_frequencies(iterable):\n",
    "    iterable_list = list(iterable)\n",
    "    counts = {item: iterable_list.count(item)\n",
    "              for item in set(iterable)}\n",
    "\n",
    "    for key, count in counts.items():\n",
    "        print(key, count)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-26T14:33:44.325400400Z",
     "start_time": "2025-05-26T14:33:44.300105Z"
    }
   },
   "id": "38b9fd1949551f7d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def compare_distributions_histogram(\n",
    "    datasets,\n",
    "    bin_count=50,\n",
    "    xlim=None,\n",
    "    labels=None,\n",
    "    title=\"Histogram Comparison\",\n",
    "    density=False,\n",
    "    alpha=0.8,\n",
    "    colors=None,\n",
    "    logx=False,\n",
    "    logy=False,\n",
    "    fig_size = (10, 6)\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare multiple distributions using side-by-side histograms.\n",
    "\n",
    "    Parameters:\n",
    "    - datasets: List of lists or arrays of numbers\n",
    "    - bin_count: Number of bins to use (even for log scale)\n",
    "    - xlim: Tuple (xmin, xmax) for x-axis limits; if None, auto-calculated\n",
    "    - labels: List of labels for each dataset\n",
    "    - title: Title of the plot\n",
    "    - density: If True, plot probability densities instead of counts\n",
    "    - alpha: Transparency of the histogram bars\n",
    "    - colors: List of colors for the histograms\n",
    "    - logx: If True, set x-axis to log scale (requires positive data)\n",
    "    - logy: If True, set y-axis to log scale\n",
    "    \"\"\"\n",
    "    datasets = [np.asarray(data) for data in datasets]\n",
    "    \n",
    "    if logx:\n",
    "        datasets = [data[data > 0] for data in datasets]\n",
    "\n",
    "    # Determine x-axis limits\n",
    "    if xlim is None:\n",
    "        xmin = min(data.min() for data in datasets)\n",
    "        xmax = max(data.max() for data in datasets)\n",
    "        xlim = (xmin, xmax)\n",
    "\n",
    "    # Define bins\n",
    "    if logx:\n",
    "        bins = np.logspace(np.log10(xlim[0]), np.log10(xlim[1]), bin_count + 1)\n",
    "    else:\n",
    "        bins = np.linspace(xlim[0], xlim[1], bin_count + 1)\n",
    "\n",
    "    # Histogram counts\n",
    "    hists = [np.histogram(data, bins=bins, density=density)[0] for data in datasets]\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    width = np.diff(bins)\n",
    "\n",
    "    num_datasets = len(datasets)\n",
    "    offsets = np.linspace(-0.4, 0.4, num=num_datasets) * width[:, None]\n",
    "\n",
    "    if labels is None:\n",
    "        labels = [f\"Data {i+1}\" for i in range(num_datasets)]\n",
    "\n",
    "    if colors is None:\n",
    "        colors = [f\"C{i}\" for i in range(num_datasets)]\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=fig_size)\n",
    "    for i, hist in enumerate(hists):\n",
    "        center_shift = offsets[:, i]\n",
    "        plt.bar(bin_centers + center_shift, hist,\n",
    "                width=width * (0.8 / num_datasets),\n",
    "                label=labels[i],\n",
    "                alpha=alpha,\n",
    "                color=colors[i % len(colors)],\n",
    "                edgecolor='black',\n",
    "                align='center')\n",
    "\n",
    "    plt.xlabel(\"Value (log scale)\" if logx else \"Value\")\n",
    "    plt.ylabel(\"Density\" if density else \"Count\")\n",
    "    plt.title(title)\n",
    "\n",
    "    # Axis scaling\n",
    "    ax = plt.gca()\n",
    "    if logx:\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.xaxis.set_major_locator(ticker.LogLocator(base=10.0, numticks=15))\n",
    "        ax.xaxis.set_minor_locator(ticker.LogLocator(base=10.0, subs=np.arange(2, 10) * 0.1,\n",
    "                                                     numticks=100))\n",
    "        ax.xaxis.set_minor_formatter(ticker.NullFormatter())\n",
    "    else:\n",
    "        ax.set_xscale(\"linear\")\n",
    "\n",
    "    if logy:\n",
    "        ax.set_yscale(\"log\")\n",
    "\n",
    "    plt.xlim(xlim)\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both' if logx or logy else 'major', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-26T14:33:44.347715Z",
     "start_time": "2025-05-26T14:33:44.324332500Z"
    }
   },
   "id": "c29870dc5cb1792a"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting [Loading the pRef and other data]\n",
      "[Loading the pRef and other data]...Finished (took 0.188740 seconds)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "the number of columns changed from 250 to 64 at row 4646; use `usecols` to select a subset and avoid this error",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m utils\u001B[38;5;241m.\u001B[39mannounce(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading the pRef and other data\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m----> 2\u001B[0m     pRef \u001B[38;5;241m=\u001B[39m get_pRef_from_vectors(name_of_vectors_file\u001B[38;5;241m=\u001B[39min_250(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmany_hot_vectors_250_random.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m      3\u001B[0m                                  name_of_fitness_file\u001B[38;5;241m=\u001B[39min_250(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfitness_250_random.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m      4\u001B[0m                                  column_in_fitness_file\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m      6\u001B[0m     train_pRef, test_pRef \u001B[38;5;241m=\u001B[39m pRef\u001B[38;5;241m.\u001B[39mtrain_test_split(test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m)\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_pRef\u001B[38;5;241m.\u001B[39msample_size\u001B[38;5;250m \u001B[39m\u001B[38;5;132;01m= }\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_pRef\u001B[38;5;241m.\u001B[39msample_size\u001B[38;5;250m \u001B[39m\u001B[38;5;132;01m= }\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\PSSearch\\PolishSystem\\read_data.py:12\u001B[0m, in \u001B[0;36mget_pRef_from_vectors\u001B[1;34m(name_of_vectors_file, name_of_fitness_file, column_in_fitness_file)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_pRef_from_vectors\u001B[39m(name_of_vectors_file: \u001B[38;5;28mstr\u001B[39m, name_of_fitness_file: \u001B[38;5;28mstr\u001B[39m, column_in_fitness_file: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m PRef:\n\u001B[1;32m---> 12\u001B[0m     full_solution_matrix \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mloadtxt(name_of_vectors_file, delimiter\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mint\u001B[39m)\n\u001B[0;32m     13\u001B[0m     fitness_array \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mgenfromtxt(name_of_fitness_file, delimiter\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mfloat\u001B[39m, usecols\u001B[38;5;241m=\u001B[39mcolumn_in_fitness_file)\n\u001B[0;32m     14\u001B[0m     search_space \u001B[38;5;241m=\u001B[39m SearchSpace(\u001B[38;5;241m2\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(full_solution_matrix\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]))\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:1356\u001B[0m, in \u001B[0;36mloadtxt\u001B[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001B[0m\n\u001B[0;32m   1353\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(delimiter, \u001B[38;5;28mbytes\u001B[39m):\n\u001B[0;32m   1354\u001B[0m     delimiter \u001B[38;5;241m=\u001B[39m delimiter\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlatin1\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m-> 1356\u001B[0m arr \u001B[38;5;241m=\u001B[39m _read(fname, dtype\u001B[38;5;241m=\u001B[39mdtype, comment\u001B[38;5;241m=\u001B[39mcomment, delimiter\u001B[38;5;241m=\u001B[39mdelimiter,\n\u001B[0;32m   1357\u001B[0m             converters\u001B[38;5;241m=\u001B[39mconverters, skiplines\u001B[38;5;241m=\u001B[39mskiprows, usecols\u001B[38;5;241m=\u001B[39musecols,\n\u001B[0;32m   1358\u001B[0m             unpack\u001B[38;5;241m=\u001B[39munpack, ndmin\u001B[38;5;241m=\u001B[39mndmin, encoding\u001B[38;5;241m=\u001B[39mencoding,\n\u001B[0;32m   1359\u001B[0m             max_rows\u001B[38;5;241m=\u001B[39mmax_rows, quote\u001B[38;5;241m=\u001B[39mquotechar)\n\u001B[0;32m   1361\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m arr\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:999\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001B[0m\n\u001B[0;32m    996\u001B[0m     data \u001B[38;5;241m=\u001B[39m _preprocess_comments(data, comments, encoding)\n\u001B[0;32m    998\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m read_dtype_via_object_chunks \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 999\u001B[0m     arr \u001B[38;5;241m=\u001B[39m _load_from_filelike(\n\u001B[0;32m   1000\u001B[0m         data, delimiter\u001B[38;5;241m=\u001B[39mdelimiter, comment\u001B[38;5;241m=\u001B[39mcomment, quote\u001B[38;5;241m=\u001B[39mquote,\n\u001B[0;32m   1001\u001B[0m         imaginary_unit\u001B[38;5;241m=\u001B[39mimaginary_unit,\n\u001B[0;32m   1002\u001B[0m         usecols\u001B[38;5;241m=\u001B[39musecols, skiplines\u001B[38;5;241m=\u001B[39mskiplines, max_rows\u001B[38;5;241m=\u001B[39mmax_rows,\n\u001B[0;32m   1003\u001B[0m         converters\u001B[38;5;241m=\u001B[39mconverters, dtype\u001B[38;5;241m=\u001B[39mdtype,\n\u001B[0;32m   1004\u001B[0m         encoding\u001B[38;5;241m=\u001B[39mencoding, filelike\u001B[38;5;241m=\u001B[39mfilelike,\n\u001B[0;32m   1005\u001B[0m         byte_converters\u001B[38;5;241m=\u001B[39mbyte_converters)\n\u001B[0;32m   1007\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1008\u001B[0m     \u001B[38;5;66;03m# This branch reads the file into chunks of object arrays and then\u001B[39;00m\n\u001B[0;32m   1009\u001B[0m     \u001B[38;5;66;03m# casts them to the desired actual dtype.  This ensures correct\u001B[39;00m\n\u001B[0;32m   1010\u001B[0m     \u001B[38;5;66;03m# string-length and datetime-unit discovery (like `arr.astype()`).\u001B[39;00m\n\u001B[0;32m   1011\u001B[0m     \u001B[38;5;66;03m# Due to chunking, certain error reports are less clear, currently.\u001B[39;00m\n\u001B[0;32m   1012\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m filelike:\n",
      "\u001B[1;31mValueError\u001B[0m: the number of columns changed from 250 to 64 at row 4646; use `usecols` to select a subset and avoid this error"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "with utils.announce(\"Loading the pRef and other data\"):\n",
    "    pRef = get_pRef_from_vectors(name_of_vectors_file=in_250(\"many_hot_vectors_250_random.csv\"),\n",
    "                                 name_of_fitness_file=in_250(\"fitness_250_random.csv\"),\n",
    "                                 column_in_fitness_file=2)\n",
    "\n",
    "    train_pRef, test_pRef = pRef.train_test_split(test_size=0.3)\n",
    "\n",
    "    print(f\"{train_pRef.sample_size = }, {test_pRef.sample_size = }\")\n",
    "    cluster_info_file_name = in_250(f\"cluster_info_250_qmc.pkl\")\n",
    "    similarities = gian_get_similarities(cluster_info_file_name)\n",
    "    n = pRef.full_solution_matrix.shape[1]\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-26T14:33:46.281445900Z",
     "start_time": "2025-05-26T14:33:44.351781600Z"
    }
   },
   "id": "dea927b86d8dbf15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with utils.announce(\"Printing the stats for each variable\"):\n",
    "    print(f\"The pRef has {pRef.sample_size}, where the variables appear\")\n",
    "    quantity_by_variable = [(index, np.sum(pRef.full_solution_matrix[:, index]))\n",
    "                            for index in range(n)]\n",
    "    top_10_most_common = heapq.nlargest(n = 10, iterable=quantity_by_variable, key=utils.second)\n",
    "    print(\"The top 10 most common vars are: \"+\", \".join(f\"{var =}:{samples = }\" for var, samples in top_10_most_common))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-26T14:33:46.287442900Z",
     "start_time": "2025-05-26T14:33:46.282445300Z"
    }
   },
   "id": "144023760f4a2a8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "with utils.announce(\"Making the operators and metrics\"):\n",
    "    custom_sampling = NCSamplerFromPRef.from_PRef(train_pRef)\n",
    "    transition_matrix = get_transition_matrix(similarities)\n",
    "    custom_crossover = NCCrossoverTransition(transition_matrix)\n",
    "    custom_mutation = NCMutationCounterproductive(transition_matrix, disappearance_probability=0.9)\n",
    "\n",
    "    # train_mean_fitness = make_mean_fitness(train_pRef, threshold=threshold)\n",
    "    train_atomicity = make_similarity_atomicity(similarities)\n",
    "    train_consistency_and_sample = make_consistency_metric_with_sample_size(train_pRef, must_match_at_least=3)\n",
    "    train_min_and_sample = make_min_metric_with_sample_size(train_pRef)\n",
    "    test_consistency_and_sample = make_consistency_metric_with_sample_size(test_pRef, must_match_at_least=3)\n",
    "\n",
    "\n",
    "\n",
    "    traditional_sampling = NCSamplerSimple.with_average_quantity(3, genome_size=n)\n",
    "    traditional_mutation = NCMutationSimple(n)\n",
    "\n",
    "    traditional_crossover = NCCrossoverSimple(swap_probability=1 / n)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-26T14:33:46.286442700Z"
    }
   },
   "id": "f9691f55e626b6f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "with utils.announce(\"Constructing the algorithm\"):\n",
    "    def get_metrics(ps: NCSolution) -> tuple[float]:\n",
    "        p_value, sample_size = train_consistency_and_sample(ps)\n",
    "        atomicity = train_atomicity(ps)\n",
    "        return (-sample_size, p_value, -atomicity)\n",
    "\n",
    "    def keep_ones_with_most_samples(population: Iterable[EvaluatedNCSolution], quantity_required: int):\n",
    "        return heapq.nsmallest(iterable=population, key=lambda x: x.fitnesses[0], n=quantity_required)\n",
    "\n",
    "    baseline_algorithm = NSGAIICustom(sampling=traditional_sampling,\n",
    "                             mutation=traditional_mutation,\n",
    "                             crossover=traditional_crossover,\n",
    "                             probability_of_crossover=0.5,\n",
    "                             eval_budget=1000,\n",
    "                             pop_size=50,\n",
    "                             tournament_size=3,\n",
    "                             mo_fitness_function=make_metrics_cached(get_metrics),\n",
    "                             unique=True,\n",
    "                             verbose=True,\n",
    "                             culler=keep_ones_with_most_samples)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-26T14:33:46.311427100Z",
     "start_time": "2025-05-26T14:33:46.289441100Z"
    }
   },
   "id": "e92bda9b97585645"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generations = list(run_NSGAII_in_steps(baseline_algorithm))\n",
    "\n",
    "\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-26T14:33:46.293442800Z"
    }
   },
   "id": "234877e2c06129f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sensible_intervals(data, intervals):\n",
    "    sorted_data = sorted(data)\n",
    "    indices_to_consider  = []\n",
    "    last_index = 0\n",
    "    while last_index < len(data):\n",
    "        indices_to_consider.append(math.floor(last_index))\n",
    "        last_index += len(data) / (intervals + 1)\n",
    "    \n",
    "    indices_to_consider.append(len(data)-1)\n",
    "    \n",
    "    to_show = [(index, sorted_data[index]) for index in indices_to_consider]\n",
    "    return to_show \n",
    "    \n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-26T14:33:46.298436300Z"
    }
   },
   "id": "598fdc069d01ec4b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "for generation_index, generation in enumerate(generations):\n",
    "    metric_arrays = np.array([ps.fitnesses for ps in generation])\n",
    "    where_non_empty = np.array([len(ps.solution) > 0 for ps in generation])\n",
    "    metric_arrays = metric_arrays[where_non_empty]\n",
    "    \n",
    "    # column 0, sample size\n",
    "    data = -metric_arrays[:, 2]\n",
    "    data = data[data < train_pRef.sample_size]\n",
    "    intervals = (sensible_intervals(data, intervals=5))\n",
    "    print(\"\\t\".join(f\"{value:>6.3}\" for value in intervals))\n",
    "    \n",
    "\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-26T14:33:46.301433300Z"
    }
   },
   "id": "ced8af23499d59a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-26T14:33:46.303432400Z"
    }
   },
   "id": "22ac8d24830e9cce"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
